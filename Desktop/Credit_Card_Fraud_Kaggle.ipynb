{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing,metrics,manifold\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\n",
    "from imblearn.over_sampling import ADASYN,SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "import keras as k\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import xgboost\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "\n",
    "#EDA\n",
    "df=pd.read_csv('../input/creditcardfraud/creditcard.csv')\n",
    "print(df)\n",
    "print(df.describe())\n",
    "print(df.columns)\n",
    "#check for NULL\n",
    "print(df.isnull().any())\n",
    "#frauds\n",
    "frauds=round(df['Class'].value_counts()[0]/len(df)*100,3)\n",
    "non_frauds=round(df['Class'].value_counts()[1]/len(df)*100,3)\n",
    "print(frauds)\n",
    "print(non_frauds)\n",
    "pd.value_counts(df['Class']).plot.bar()\n",
    "plt.title(\"Fraud detection freq vs class\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Freq\")\n",
    "df['Class'].value_counts()\n",
    "plt.show()\n",
    "#check for skewed\n",
    "fig,ax= plt.subplots(1,2,figsize=(20,4))\n",
    "amount=df['Amount'].values\n",
    "time=df['Time'].values\n",
    "sns.distplot(amount,ax=ax[0],color='g')\n",
    "ax[0].set_title(\"Amount\")\n",
    "ax[0].set_xlim(min(df['Amount']),max(df['Amount']))\n",
    "\n",
    "sns.distplot(time,ax=ax[1],color='b')\n",
    "ax[1].set_title(\"Time\")\n",
    "ax[1].set_xlim(min(df['Time']),max(df['Time']))\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "robscaler=RobustScaler()\n",
    "df['Amount']=robscaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['Time']=robscaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "X=df.drop('Class',axis=1)\n",
    "Y=df['Class']\n",
    "\n",
    "\n",
    "#dimen reduction\n",
    "tsne=TSNE(n_components=2,random_state=42).fit_transform(X.values)\n",
    "pca=PCA(n_components=2,random_state=42).fit_transform(X.values)\n",
    "tuncated_svd=TruncatedSVD(n_components=2,algorithm='randomized',random_state=42).fit_transform(X.values)\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n",
    "f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n",
    "\n",
    "\n",
    "blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n",
    "red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n",
    "\n",
    "\n",
    "# t-SNE scatter plot\n",
    "ax1.scatter(tsne[:,0],tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax1.scatter(tsne[:,0],tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax1.set_title('t-SNE', fontsize=14)\n",
    "\n",
    "ax1.grid(True)\n",
    "\n",
    "ax1.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "\n",
    "# PCA scatter plot\n",
    "ax2.scatter(pca[:,0],pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax2.scatter(pca[:,0],pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax2.set_title('PCA', fontsize=14)\n",
    "\n",
    "ax2.grid(True)\n",
    "\n",
    "ax2.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# TruncatedSVD scatter plot\n",
    "ax3.scatter(truncated_svd[:,0], truncated_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax3.scatter(truncated_svd[:,0], truncated_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax3.set_title('Truncated SVD', fontsize=14)\n",
    "\n",
    "ax3.grid(True)\n",
    "\n",
    "ax3.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"X shape\",X.shape)\n",
    "print(\"Y shape\",Y.shape)\n",
    "#sandard split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=0)\n",
    "#stratified split\n",
    "s=StratifiedKFold(n_splits=5,random_state=0,shuffle=False)\n",
    "for train_idx,test_idx in s.split(X,Y):\n",
    "  orig_Xtrain,orig_Xtest=X.iloc[train_idx],X.iloc[test_idx]\n",
    "  orig_Ytrain,orig_Ytest=Y.iloc[train_idx],Y.iloc[test_idx]\n",
    "orig_Xtrain=orig_Xtrain.values\n",
    "orig_Xtest=orig_Xtest.values\n",
    "orig_Ytrain=orig_Ytrain.values\n",
    "orig_Ytest=orig_Ytest.values\n",
    "\n",
    "train_unique_label,train_counts_label=np.unique(orig_Ytrain,return_counts=True)\n",
    "test_unique_label,test_counts_label=np.unique(orig_Ytest,return_counts=True)\n",
    "print(train_unique_label)\n",
    "print(test_unique_label)\n",
    "print(train_counts_label/len(orig_Ytrain))\n",
    "print(test_counts_label/len(orig_Ytest))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#oversampling as class distinction is biased towards one side\n",
    "print(df.columns)\n",
    "x=np.array(df.iloc[:,df.columns != 'Class'])\n",
    "y=np.array(df.iloc[:,df.columns == 'Class'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"X shape\",X.shape)\n",
    "print(\"Y shape\",Y.shape)\n",
    "#sandard split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#smote\n",
    "smote=SMOTE(random_state=2)\n",
    "x_residual_train,y_residual_train=smote.fit_resample(X_train,Y_train)\n",
    "print(\"Up sampled smote\")\n",
    "print(x_residual_train.shape)\n",
    "print(y_residual_train.shape)\n",
    "print(\"no of frauds and non frauds\")\n",
    "print(sum(y_residual_train==1))\n",
    "#print(y_residual_train.value_counts()[1])\n",
    "\n",
    "#adasyn sampling\n",
    "adasyn=ADASYN(sampling_strategy=\"minority\",random_state=420, n_neighbors=5)\n",
    "x_residual_train_adasyn,y_residual_train_adasyn=adasyn.fit_resample(X_train,Y_train)\n",
    "print(\"Up sampled adasyn\")\n",
    "print(x_residual_train_adasyn.shape)\n",
    "print(y_residual_train_adasyn.shape)\n",
    "print(\"no of frauds and non frauds\")\n",
    "print(sum(y_residual_train==1))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#k fold for cross validation score generation \n",
    "models=[]\n",
    "models.append(('LR',LogisticRegression()))\n",
    "models.append(('KNN',KNeighborsClassifier()))\n",
    "models.append(('SVC',SVC()))\n",
    "models.append(('LDA',LinearDiscriminantAnalysis()))\n",
    "models.append(('DT',DecisionTreeClassifier()))\n",
    "model_result=[]\n",
    "scoring='accuracy'\n",
    "for name,model in models:\n",
    "    kfold=KFold(n_splits=10,random_state=7)\n",
    "    results=cross_val_score(model,x_residual_train_adasyn,y_residual_train_adasyn,cv=kfold,scoring=scoring)\n",
    "    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n",
    "    model_result.append(results.mean())\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hyperparameter tuning \n",
    "logistic_reg_params={\"penalty\":['l1','l2'],'C':[0.0001,0.001,0.01,1,10,100,1000]}\n",
    "gridlog_search=GridSearchCV(LogisticRegression(),logistic_reg_params)\n",
    "gridlog_search.fit(x_residual_train_adasyn,y_residual_train_adasyn)\n",
    "log_reg=gridlog_search.best_estimator_\n",
    "\n",
    "knc_params={\"n_neighbors\":list(range(2,5,1)),'algorithm':['auto','ball_tree','kd_tree','brute']}\n",
    "gridknc_search=GridSearchCV(KNeighborsClassifier(),knc_params)\n",
    "gridknc_search.fit(x_residual_train_adasyn,y_residual_train_adasyn)\n",
    "knc_grid=gridknc_search.best_estimator_\n",
    "\n",
    "svc_params={'C':[0.5,0.7,0.9,0.1],'kernel':['rbf','poly','sigmoid','linear']}\n",
    "gridsvc_search=GridSearchCV(SVC(),svc_params)\n",
    "gridsvc_search.fit(x_residual_train_adasyn,y_residual_train_adasyn)\n",
    "svc_grid=gridsvc_search.best_estimator_\n",
    "\n",
    "dec_tree_params={'criterion':['gini','entropy'],'maxdepth'=list(range(2,4,1))}\n",
    "griddec_search=GridSearchCV(DecisionTreeClassifier(),dec_tree_params)\n",
    "griddec_search.fit(x_residual_train_adasyn,y_residual_train_adasyn)\n",
    "dectree_grid=grddec_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k fold validatio of the models\n",
    "for name,model in models:\n",
    "    kfold=KFold(n_splits=10,random_state=7)\n",
    "    acc=cross_val_predict(model,x_residual_train_adasyn,y_residual_train_adasyn,cv=kfold,method=\"decision_function\")\n",
    "    print(\"Classifiers: \",name, \"Has a training score of\", round(acc.mean(), 2) * 100, \"% accuracy score\")\n",
    "            \n",
    "#prediction\n",
    "for name,model in models:\n",
    "    y_pred_adasyn=model.predict(x_residual_train_adasyn)\n",
    "    cnf_matrix=confusion_matrix(y_residual_train_adasyn,y_pred_adasyn)\n",
    "    print(classification_report(y_residual_train_adasyn,y_pred_adasyn))\n",
    "    #validation\n",
    "    y_pred_test_adasyn=model.predict(X_test)\n",
    "    cnf_matrix=confusion_matrix(Y_test,y_pred_test_adasyn)\n",
    "    print(classification_report(Y_test,y_pred_test_adasyn))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep learning 4 intermediate hidden\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy,binary_crossentropy\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "n_inp=np.array(X_train)\n",
    "\n",
    "model=Sequential([Dense(units=16,input_dim=30,activation=\"relu\"),\n",
    "                  Dense(units=32,activation=\"relu\"),\n",
    "                  Dropout(0.5),\n",
    "                  Dense(units=20,activation=\"relu\"),\n",
    "                  Dense(units=16,activation=\"relu\"),\n",
    "                  Dense(units=1,activation=\"sigmoid\")\n",
    "                 \n",
    "                 ])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_residual_train_adasyn,y_residual_train_adasyn,batch_size=25,epochs=5,shuffle=False,verbose=2)\n",
    "score=model.evaluate(X_test,Y_test)\n",
    "print(\"Score\")\n",
    "print(score)\n",
    "y_deep_predict=model.predict(X_test)\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_matrix(Y_test,y_deep_predict.round()))\n",
    "print(1-accuracy_score(Y_test,y_deep_predict.round()))\n",
    "                                                                        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#deep learning 4 intermediate hidden\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy,binary_crossentropy\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "n_inp=np.array(X_train)\n",
    "\n",
    "model=Sequential([Dense(units=16,input_dim=30,activation=\"relu\"),\n",
    "                  Dense(units=32,activation=\"relu\"),\n",
    "                  Dropout(0.5),\n",
    "                  Dense(units=20,activation=\"relu\"),\n",
    "                  Dense(units=16,activation=\"relu\"),\n",
    "                  Dense(units=1,activation=\"sigmoid\")\n",
    "                 \n",
    "                 ])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(x_residual_train_adasyn,y_residual_train_adasyn,batch_size=25,epochs=5,shuffle=False,verbose=2)\n",
    "score=model.evaluate(X_test,Y_test)\n",
    "print(\"Score\")\n",
    "print(score)\n",
    "y_deep_predict=model.predict(X_test)\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_matrix(Y_test,y_deep_predict.round()))\n",
    "print(accuracy_score(Y_test,y_deep_predict.round()))\n",
    "                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xbgboost classifier\n",
    "#boosting trees\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from xgboost import XGBClassifier as xg\n",
    "model_xgb= xg(n_estimators=100,random_state=42)\n",
    "model_xgb.fit(x_residual_train_adasyn,y_residual_train_adasyn)\n",
    "y_pred_lgbm=model_xgb.predict(X_test)\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_matrix(Y_test,y_pred_xgb))\n",
    "print(accuracy_score(Y_test,y_pred_xgb.round()))\n",
    "                                                                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score \n",
    "from lightgbm import LGBMClassifier as lg\n",
    "model_lgbm= lg(n_estimators=100,random_state=42)\n",
    "model_lgbm.fit(x_residual_train_adasyn,y_residual_train_adasyn)\n",
    "y_pred_lgbm=model_lgbm.predict(X_test)\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_matrix(Y_test,y_pred_lgbm))\n",
    "print(accuracy_score(Y_test,y_pred_lgbm.round()))\n",
    "                                                                        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
